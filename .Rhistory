make
getwd()
setwd("/Users/june/Documents/Git/data550_midterm_proj")
renv::init()
getwd()
setwd("/Users/june/Documents/Git/data550_midterm_proj")
load("MCI_MRI.RData")
# Load necessary library
library(glmnet)
# Assuming `x` is a matrix and `y` is a vector from MCI_MRI.RData
# Convert x to matrix format (if not already)
x_matrix <- as.matrix(x)
# Fit logistic regression
model <- glm(y ~ x_matrix, family = binomial)
# Check warnings during the fit
warnings()
# Load necessary library
library(ggplot2)
# Perform PCA on x
pca_result <- prcomp(x, scale. = TRUE)
# Extract eigenvalues (variance explained by each principal component)
eigenvalues <- (pca_result$sdev)^2
# Create a data frame for the scree plot
scree_data <- data.frame(
Component = 1:length(eigenvalues),
Eigenvalue = eigenvalues
)
# Create the scree plot using ggplot2
ggplot(scree_data, aes(x = Component, y = Eigenvalue)) +
geom_line() +
geom_point() +
labs(
title = "Scree Plot of PCA Eigenvalues",
x = "Principal Component",
y = "Eigenvalue"
) +
theme_minimal()
# Subset the first 20 eigenvalues
scree_data_20 <- scree_data[1:20, ]
# Create the scree plot for the first 20 eigenvalues
ggplot(scree_data_20, aes(x = Component, y = Eigenvalue)) +
geom_line() +
geom_point() +
labs(
title = "Scree Plot of First 20 PCA Eigenvalues",
x = "Principal Component",
y = "Eigenvalue"
) +
theme_minimal()
View(scree_data_20)
# Calculate cumulative variance explained by the first 4 components
total_variance <- sum(eigenvalues)
variance_prior_to_elbow <- sum(eigenvalues[1:4]) / total_variance
# Print the result
variance_prior_to_elbow
# Calculate the proportion of variance explained by each component
proportion_variance <- eigenvalues / sum(eigenvalues)
# Calculate the cumulative variance explained
cumulative_variance <- cumsum(proportion_variance)
# Find the number of components required to explain 90% of the variance
components_90 <- which(cumulative_variance >= 0.90)[1]
# Print the result
components_90
# Assuming `labels` is a vector of categorical data corresponding to the columns of `x`
# Extract loadings (principal component scores)
pc1 <- pca_result$x[, 1]  # First principal component
pc2 <- pca_result$x[, 2]  # Second principal component
# Create a data frame for plotting
scatter_data <- data.frame(
PC1 = pc1,
PC2 = pc2,
Label = labels  # Assuming labels is defined
)
View(x)
View(x_matrix)
View(x_matrix)
View(pca_result)
load("MCI_MRI.RData")
View(x)
View(x_matrix)
# Assuming `labels` is a vector of categorical data corresponding to the columns of `x`
# Extract loadings (principal component scores)
pc1 <- pca_result$x[, 1]  # First principal component
pc2 <- pca_result$x[, 2]  # Second principal component
# Create a data frame for plotting
scatter_data <- data.frame(
PC1 = pc1,
PC2 = pc2,
Label = labels  # Assuming labels is defined
)
# Extract loadings for features (columns of `x`) for PC1 and PC2
loadings_pc1 <- pca_result$rotation[, 1]  # Loadings for PC1
loadings_pc2 <- pca_result$rotation[, 2]  # Loadings for PC2
# Create a data frame for plotting
scatter_data_features <- data.frame(
PC1 = loadings_pc1,
PC2 = loadings_pc2,
Label = labels  # Assuming labels correspond to columns of `x`
)
# Plot the loadings of PC1 vs PC2
ggplot(scatter_data_features, aes(x = PC1, y = PC2, color = Label)) +
geom_point(alpha = 0.7, size = 2) +
labs(
title = "Scatter Plot of PC1 vs PC2 Loadings (Features)",
x = "Principal Component 1",
y = "Principal Component 2",
color = "Label"
) +
theme_minimal() +
theme(legend.position = "right")
# Extract the PCA scores for observations
pca_scores <- as.data.frame(pca_result$x)
# Decide on the number of components (e.g., 4 based on elbow, or calculate 90% variance)
num_components <- components_90  # Replace with the desired number of components
# Subset the scores for the chosen number of components
selected_scores <- pca_scores[, 1:num_components]
# Add the outcome variable y to the data
pca_regression_data <- cbind(selected_scores, Diagnosis = y)
# Fit a logistic regression model using the selected principal components
pca_regression_model <- glm(Diagnosis ~ ., data = pca_regression_data, family = binomial)
# Print the summary of the model
summary(pca_regression_model)
# Extract the PCA scores for observations
pca_scores <- as.data.frame(pca_result$x)
# Decide on the number of components
num_components <- 4  # Replace with the desired number of components
# Subset the scores for the chosen number of components
selected_scores <- pca_scores[, 1:num_components]
# Add the outcome variable y to the data
pca_regression_data <- cbind(selected_scores, Diagnosis = y)
# Fit a logistic regression model using the selected principal components
pca_regression_model <- glm(Diagnosis ~ ., data = pca_regression_data, family = binomial)
# Print the summary of the model
summary(pca_regression_model)
# Select the most significant component (PC2 in this case)
loadings_pc2 <- pca_result$rotation[, 2]  # Loadings for PC2
# Adjust the sign of the loadings for interpretability (if the largest loading is negative)
if (max(abs(loadings_pc2)) == -max(loadings_pc2)) {
loadings_pc2 <- -loadings_pc2
}
# Create a data frame for plotting
loading_data <- data.frame(
Index = 1:length(loadings_pc2),
Loading = loadings_pc2,
Label = labels  # Assuming labels correspond to the columns of `x`
)
# Plot the loadings versus the index number, colored by labels
library(ggplot2)
ggplot(loading_data, aes(x = Index, y = Loading, color = Label)) +
geom_point(alpha = 0.7, size = 2) +
labs(
title = "Loadings of PC2 vs Index Number",
x = "Index",
y = "PC2 Loadings",
color = "Label"
) +
theme_minimal() +
theme(legend.position = "right")
# Load the car package for VIF calculation
library(car)
# Calculate VIFs for the PCA regression model
vif_values <- vif(pca_regression_model)
# Print the VIFs
vif_values
# Load glmnet package
library(glmnet)
# Ensure x is a matrix and y is a binary response variable
x_matrix <- as.matrix(x)  # Convert x to a matrix if not already
y_binary <- as.factor(y)  # Ensure y is treated as a binary factor
# Fit LASSO regression with glmnet
lasso_model <- glmnet(x_matrix, y_binary, family = "binomial", alpha = 1)
# Plot the coefficient paths
plot(lasso_model, xvar = "lambda", label = TRUE)
# Add labels for clarity
title(main = "LASSO Coefficient Paths")
# Set seed for reproducibility
set.seed(777)
# Perform 10-fold cross-validation for LASSO
cv_lasso <- cv.glmnet(x_matrix, y_binary, family = "binomial", alpha = 1)
# Plot the cross-validation results
plot(cv_lasso)
# Add labels for clarity
title(main = "10-Fold Cross-Validation for LASSO")
# Extract coefficients at lambda.min (optimal lambda)
coefficients_lambda_min <- coef(cv_lasso, s = "lambda.min")
# Convert coefficients to a data frame and exclude the intercept
coefficients_df <- as.data.frame(as.matrix(coefficients_lambda_min))
coefficients_df <- coefficients_df[-1, ]  # Remove the intercept (first row)
# Add index and labels
coefficients_df$Index <- 1:nrow(coefficients_df)
# Extract coefficients at lambda.min (optimal lambda)
coefficients_lambda_min <- coef(cv_lasso, s = "lambda.min")
# Convert coefficients to a data frame and exclude the intercept
coefficients_df <- as.data.frame(as.matrix(coefficients_lambda_min))
coefficients_df <- coefficients_df[-1, ]  # Remove the intercept (first row)
# Add index and labels
coefficients_df$Index <- 1:nrow(coefficients_df)
view(coefficients_df)
print(coefficients_df)
# Extract coefficients at lambda.min (optimal lambda)
coefficients_lambda_min <- coef(cv_lasso, s = "lambda.min")
# Convert the sparse matrix to a data frame
coefficients_df <- as.data.frame(as.matrix(coefficients_lambda_min))
# Add a column for predictor names
coefficients_df$Feature <- rownames(coefficients_df)
# Remove the intercept
coefficients_df <- coefficients_df[coefficients_df$Feature != "(Intercept)", ]
# Add index and labels
coefficients_df$Index <- 1:nrow(coefficients_df)
coefficients_df$Label <- labels  # Assuming `labels` corresponds to the features in `x`
# Rename column for readability
colnames(coefficients_df)[1] <- "Coefficient"
# Load ggplot2 for visualization
library(ggplot2)
# Create the plot
ggplot(coefficients_df, aes(x = Index, y = Coefficient, color = Label)) +
geom_point(size = 2, alpha = 0.7) +
labs(
title = "Coefficients at Lambda.min vs Index Number",
x = "Index",
y = "Coefficient Value",
color = "Label"
) +
theme_minimal() +
theme(legend.position = "right")
# Count the number of non-zero coefficients at lambda.min
non_zero_coefficients <- sum(coefficients_lambda_min != 0) - 1  # Exclude intercept
non_zero_coefficients
# Extract coefficients at lambda.min as a numeric vector
coefficients_vector <- as.matrix(coefficients_lambda_min)[, 1]
# Exclude the intercept
coefficients_vector <- coefficients_vector[-1]
# Identify the variable with the largest absolute coefficient
max_coefficient_index <- which.max(abs(coefficients_vector))
max_variable <- rownames(coefficients_lambda_min)[max_coefficient_index + 1]  # Adjust for intercept
max_coefficient <- coefficients_vector[max_coefficient_index]
# Transform the coefficient to the odds scale
odds_ratio <- exp(max_coefficient)
# Print the results
list(
Variable = max_variable,
Coefficient = max_coefficient,
Odds_Ratio = odds_ratio
)
# Set a new seed
set.seed(123)
# Re-run 10-fold cross-validation for LASSO
cv_lasso_new <- cv.glmnet(x_matrix, y_binary, family = "binomial", alpha = 1)
# Plot the new cross-validation results
plot(cv_lasso_new)
title(main = "10-Fold Cross-Validation with Seed 123")
# Compare lambda.min values
lambda_min_old <- cv_lasso$lambda.min
lambda_min_new <- cv_lasso_new$lambda.min
list(
Lambda_Min_Old = lambda_min_old,
Lambda_Min_New = lambda_min_new,
Did_Change = lambda_min_old != lambda_min_new
)
# Fit Elastic Net with alpha = 0.5
elastic_net_model <- glmnet(x_matrix, y_binary, family = "binomial", alpha = 0.5)
# Plot the coefficient paths for the Elastic Net model
plot(elastic_net_model, xvar = "lambda", label = TRUE)
# Add title to the plot for clarity
title(main = "Elastic Net Coefficient Paths (Alpha = 0.5)")
# Set the seed for reproducibility
set.seed(777)
# Perform 10-fold cross-validation for Elastic Net with alpha = 0.5
cv_elastic_net <- cv.glmnet(x_matrix, y_binary, family = "binomial", alpha = 0.5)
# Plot the cross-validation results
plot(cv_elastic_net)
title(main = "10-Fold Cross-Validation for Elastic Net (Alpha = 0.5)")
# Set the seed for reproducibility
set.seed(777)
# Perform 10-fold cross-validation for Elastic Net with alpha = 0.5
cv_elastic_net <- cv.glmnet(x_matrix, y_binary, family = "binomial", alpha = 0.5,
lambda = seq(0.001, 0.1, length = 100)
)
# Plot the cross-validation results
plot(cv_elastic_net)
title(main = "10-Fold Cross-Validation for Elastic Net (Alpha = 0.5)")
# Set the seed for reproducibility
set.seed(777)
# Perform 10-fold cross-validation for Elastic Net with alpha = 0.5
cv_elastic_net <- cv.glmnet(x_matrix, y_binary, family = "binomial", alpha = 0.5)
# Plot the cross-validation results
plot(cv_elastic_net)
title(main = "10-Fold Cross-Validation for Elastic Net (Alpha = 0.5)")
# Extract coefficients at lambda.min for Elastic Net
coefficients_en_lambda_min <- coef(cv_elastic_net, s = "lambda.min")
# Convert to a data frame and exclude the intercept
coefficients_en_df <- as.data.frame(as.matrix(coefficients_en_lambda_min))
coefficients_en_df <- coefficients_en_df[-1, ]  # Remove intercept
# Add index and labels
coefficients_en_df$Index <- 1:nrow(coefficients_en_df)
# Extract coefficients at lambda.min for Elastic Net
coefficients_en_lambda_min <- coef(cv_elastic_net, s = "lambda.min")
# Convert the sparse matrix to a data frame
coefficients_en_df <- as.data.frame(as.matrix(coefficients_en_lambda_min))
# Add a column for predictor names
coefficients_en_df$Feature <- rownames(coefficients_en_lambda_min)
# Remove the intercept
coefficients_en_df <- coefficients_en_df[coefficients_en_df$Feature != "(Intercept)", ]
# Add index and labels
coefficients_en_df$Index <- 1:nrow(coefficients_en_df)
coefficients_en_df$Label <- labels  # Assuming labels correspond to columns of x
# Rename coefficient column for readability
colnames(coefficients_en_df)[1] <- "Coefficient"
# Plot the coefficients at lambda.min
library(ggplot2)
ggplot(coefficients_en_df, aes(x = Index, y = Coefficient, color = Label)) +
geom_point(size = 2, alpha = 0.7) +
labs(
title = "Elastic Net Coefficients at Lambda.min vs Index Number",
x = "Index",
y = "Coefficient Value",
color = "Label"
) +
theme_minimal() +
theme(legend.position = "right")
# Count the number of non-zero coefficients at lambda.min for Elastic Net
non_zero_coefficients_en <- sum(coefficients_en_lambda_min != 0) - 1  # Exclude the intercept
non_zero_coefficients_en
# Extract coefficients at lambda.1se for Elastic Net
coefficients_en_lambda_1se <- coef(cv_elastic_net, s = "lambda.1se")
# Convert the sparse matrix to a data frame
coefficients_1se_df <- as.data.frame(as.matrix(coefficients_en_lambda_1se))
# Add a column for predictor names
coefficients_1se_df$Feature <- rownames(coefficients_en_lambda_1se)
# Remove the intercept
coefficients_1se_df <- coefficients_1se_df[coefficients_1se_df$Feature != "(Intercept)", ]
# Add index and labels
coefficients_1se_df$Index <- 1:nrow(coefficients_1se_df)
coefficients_1se_df$Label <- labels  # Assuming labels correspond to columns of x
# Rename coefficient column for readability
colnames(coefficients_1se_df)[1] <- "Coefficient"
# Plot the coefficients at lambda.1se
library(ggplot2)
ggplot(coefficients_1se_df, aes(x = Index, y = Coefficient, color = Label)) +
geom_point(size = 2, alpha = 0.7) +
labs(
title = "Elastic Net Coefficients at Lambda.1se vs Index Number",
x = "Index",
y = "Coefficient Value",
color = "Label"
) +
theme_minimal() +
theme(legend.position = "right")
# Extract coefficients at lambda.1se and remove the intercept
coefficients_vector_1se <- as.matrix(coefficients_en_lambda_1se)[-1, ]
# Identify the variable with the largest absolute coefficient
max_coeff_index_1se <- which.max(abs(coefficients_vector_1se))
max_variable_1se <- rownames(coefficients_en_lambda_1se)[max_coeff_index_1se + 1]
max_coefficient_1se <- coefficients_vector_1se[max_coeff_index_1se]
# Transform to odds scale
odds_ratio_1se <- exp(max_coefficient_1se)
# Output the variable, coefficient, and odds ratio
list(
Variable = max_variable_1se,
Coefficient = max_coefficient_1se,
Odds_Ratio = odds_ratio_1se
)
# Extract the names of variables with non-zero coefficients at lambda.1se
selected_variables <- rownames(coefficients_en_lambda_1se)[which(coefficients_en_lambda_1se != 0)]
selected_variables <- selected_variables[selected_variables != "(Intercept)"]  # Exclude the intercept
# Subset the dataset to include only selected variables
x_selected <- x_matrix[, selected_variables, drop = FALSE]
# Fit a GLM using only the selected variables
glm_model <- glm(y_binary ~ ., data = as.data.frame(cbind(x_selected, y_binary)), family = binomial)
# Extract the coefficient of the variable identified in 4.6 from the GLM summary
glm_coefficients <- coef(glm_model)
# Extract the names of variables with non-zero coefficients at lambda.1se
selected_variables <- rownames(coefficients_en_lambda_1se)[which(coefficients_en_lambda_1se != 0)]
selected_variables <- selected_variables[selected_variables != "(Intercept)"]  # Exclude the intercept
# Ensure y_binary is numeric and coded as 0 and 1
y_binary <- as.numeric(y)  # Convert y to numeric if necessary
# Subset the dataset to include only selected variables
x_selected <- x_matrix[, selected_variables, drop = FALSE]
# Fit a GLM using only the selected variables
glm_model <- glm(y_binary ~ ., data = as.data.frame(cbind(x_selected, y_binary)), family = binomial)
# Print the summary of the GLM
summary(glm_model)
# Extract the coefficient of the variable identified in 4.6 from the GLM summary
glm_coefficients <- coef(glm_model)
variable_coefficient <- glm_coefficients[max_variable_1se]
# Transform to odds scale
odds_ratio_glm <- exp(variable_coefficient)
# Output the coefficient and odds ratio
list(
Variable = max_variable_1se,
GLM_Coefficient = variable_coefficient,
Odds_Ratio = odds_ratio_glm
)
# Extract the coefficients from the fitted GLM model
glm_coefficients <- coef(glm_model)
# Get the coefficient for the variable identified in 4.6
variable_coefficient <- glm_coefficients[max_variable_1se]
# Transform the coefficient to the odds scale
odds_ratio_glm <- exp(variable_coefficient)
# Output the variable name, coefficient, and odds ratio
list(
Variable = max_variable_1se,
GLM_Coefficient = variable_coefficient,
Odds_Ratio = odds_ratio_glm
)
# Identify the variable of interest (replace with the actual variable name if different)
variable_of_interest <- "`Volume (WM Parcellation) of LeftHippocampus`"
# Extract the coefficient for the variable from the GLM summary
glm_coefficients <- coef(glm_model)
variable_coefficient <- glm_coefficients[variable_of_interest]
# Transform the coefficient to the odds scale
odds_ratio_glm <- exp(variable_coefficient)
# Output the results
list(
Variable = variable_of_interest,
GLM_Coefficient = variable_coefficient,
Odds_Ratio = odds_ratio_glm
)
